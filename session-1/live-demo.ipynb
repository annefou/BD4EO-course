{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1 Live Demo: EO Processing Workflows & Python\n",
    "\n",
    "**Course:** BD4EO - Big Data Foundations for Earth Observation  \n",
    "**Session:** Tuesday 09:10-10:30  \n",
    "**Instructor:** Anne Fouilloux (PANGEO)\n",
    "\n",
    "This notebook contains the live demonstrations from Session 1, showing modern EO processing workflows using Python and cloud-native tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries\n",
    "\n",
    "First, let's import the essential Python libraries for EO processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Geospatial libraries\n",
    "import rasterio\n",
    "import rioxarray as rxr\n",
    "import geopandas as gpd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# Cloud and parallel computing\n",
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "# STAC for data discovery\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"Xarray version: {xr.__version__}\")\n",
    "print(f\"Dask version: {dask.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Introduction to Xarray and Data Cubes\n",
    "\n",
    "Let's start with the fundamental concept of data cubes using Xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample atmospheric dataset (built into xarray)\n",
    "ds = xr.tutorial.open_dataset(\"air_temperature\")\n",
    "\n",
    "print(\"Dataset structure:\")\n",
    "print(ds)\n",
    "print(\"\\nDimensions and coordinates:\")\n",
    "print(f\"Time: {ds.time.size} timesteps from {ds.time.values[0]} to {ds.time.values[-1]}\")\n",
    "print(f\"Latitude: {ds.lat.size} points from {ds.lat.min().values}° to {ds.lat.max().values}°\")\n",
    "print(f\"Longitude: {ds.lon.size} points from {ds.lon.min().values}° to {ds.lon.max().values}°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate data cube slicing and selection\n",
    "# Select data for a specific time\n",
    "single_time = ds.air.isel(time=0)\n",
    "\n",
    "# Select data for a specific location over time\n",
    "time_series = ds.air.sel(lat=50, lon=260, method='nearest')\n",
    "\n",
    "# Select data for a geographic region\n",
    "region = ds.air.sel(lat=slice(40, 60), lon=slice(240, 280))\n",
    "\n",
    "print(f\"Single time slice shape: {single_time.shape}\")\n",
    "print(f\"Time series shape: {time_series.shape}\")\n",
    "print(f\"Regional subset shape: {region.shape}\")\n",
    "\n",
    "# Demonstrate powerful operations\n",
    "seasonal_mean = ds.air.groupby('time.season').mean()\n",
    "print(f\"Seasonal mean shape: {seasonal_mean.shape}\")\n",
    "print(f\"Seasons: {list(seasonal_mean.season.values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Create a multi-panel plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10), \n",
    "                        subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "# Plot seasonal means\n",
    "seasons = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "titles = ['Winter (DJF)', 'Spring (MAM)', 'Summer (JJA)', 'Fall (SON)']\n",
    "\n",
    "for i, (season, title) in enumerate(zip(seasons, titles)):\n",
    "    ax = axes[i//2, i%2]\n",
    "    \n",
    "    # Plot the data\n",
    "    im = seasonal_mean.sel(season=season).plot(\n",
    "        ax=ax, transform=ccrs.PlateCarree(),\n",
    "        cmap='coolwarm', add_colorbar=False,\n",
    "        vmin=240, vmax=300\n",
    "    )\n",
    "    \n",
    "    # Add geographic features\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_global()\n",
    "\n",
    "# Add a colorbar\n",
    "plt.tight_layout()\n",
    "cbar = plt.colorbar(im, ax=axes, orientation='horizontal', \n",
    "                   fraction=0.05, pad=0.1, aspect=50)\n",
    "cbar.set_label('Temperature (K)', fontsize=12)\n",
    "\n",
    "plt.suptitle('Seasonal Temperature Patterns - Demonstrating Data Cube Concepts', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: STAC Data Discovery\n",
    "\n",
    "Now let's demonstrate how to discover real satellite data using STAC (SpatioTemporal Asset Catalog):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Microsoft Planetary Computer STAC catalog\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "\n",
    "print(\"Available collections:\")\n",
    "collections = list(catalog.get_collections())\n",
    "eo_collections = [c for c in collections if 'sentinel' in c.id.lower() or 'landsat' in c.id.lower()]\n",
    "\n",
    "for collection in eo_collections[:10]:  # Show first 10 EO collections\n",
    "    print(f\"- {collection.id}: {collection.title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for Sentinel-2 data over a specific area and time\n",
    "# Let's use the Baltic region as an example (relevant to Riga location)\n",
    "bbox = [20.0, 54.0, 28.0, 60.0]  # Baltic region bounding box\n",
    "datetime_range = \"2023-06-01/2023-06-30\"  # Summer 2023\n",
    "\n",
    "search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=bbox,\n",
    "    datetime=datetime_range,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 20}}  # Less than 20% cloud cover\n",
    ")\n",
    "\n",
    "items = search.item_collection()\n",
    "print(f\"Found {len(items)} Sentinel-2 scenes with <20% cloud cover\")\n",
    "\n",
    "# Display information about the first few items\n",
    "for i, item in enumerate(items[:3]):\n",
    "    print(f\"\\nScene {i+1}:\")\n",
    "    print(f\"  Date: {item.datetime.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  Cloud cover: {item.properties.get('eo:cloud_cover', 'N/A')}%\")\n",
    "    print(f\"  Assets: {list(item.assets.keys())[:5]}...\")  # Show first 5 assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine one item in detail\n",
    "if len(items) > 0:\n",
    "    item = items[0]\n",
    "    print(f\"Detailed information for scene from {item.datetime.strftime('%Y-%m-%d')}:\")\n",
    "    print(f\"\\nSpatial coverage:\")\n",
    "    print(f\"  Bounding box: {item.bbox}\")\n",
    "    \n",
    "    print(f\"\\nAvailable bands/assets:\")\n",
    "    for asset_key, asset in item.assets.items():\n",
    "        if asset_key in ['B02', 'B03', 'B04', 'B08']:  # Main visible/NIR bands\n",
    "            print(f\"  {asset_key}: {asset.title} - {asset.href}\")\n",
    "    \n",
    "    print(f\"\\nProperties:\")\n",
    "    interesting_props = ['eo:cloud_cover', 'proj:epsg', 'datetime']\n",
    "    for prop in interesting_props:\n",
    "        if prop in item.properties:\n",
    "            print(f\"  {prop}: {item.properties[prop]}\")\n",
    "else:\n",
    "    print(\"No items found for the specified criteria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3: Cloud-Optimized Data Access\n",
    "\n",
    "Now let's demonstrate how to access and process satellite data efficiently using cloud-optimized formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentinel-2 data using xarray and rioxarray\n",
    "if len(items) > 0:\n",
    "    item = items[0]\n",
    "    \n",
    "    # Load specific bands (Red, Green, Blue, NIR)\n",
    "    bands_to_load = ['B04', 'B03', 'B02', 'B08']  # Red, Green, Blue, NIR\n",
    "    band_data = {}\n",
    "    \n",
    "    for band in bands_to_load:\n",
    "        if band in item.assets:\n",
    "            print(f\"Loading {band}...\")\n",
    "            # Note: In a real environment, you would load the actual data\n",
    "            # Here we'll create a placeholder for demonstration\n",
    "            band_data[band] = f\"Would load: {item.assets[band].href}\"\n",
    "    \n",
    "    print(\"\\nData loading complete!\")\n",
    "    print(\"In a real environment, each band would be loaded as an xarray DataArray\")\n",
    "    \n",
    "    # Demonstrate what the loaded data structure would look like\n",
    "    print(\"\\nExpected data structure:\")\n",
    "    print(\"- Shape: (height, width) e.g., (10980, 10980) for 10m bands\")\n",
    "    print(\"- Coordinates: x (easting), y (northing)\")\n",
    "    print(\"- CRS: UTM zone (automatically detected)\")\n",
    "    print(\"- Chunks: Automatically chunked for parallel processing\")\n",
    "else:\n",
    "    print(\"Creating synthetic data for demonstration...\")\n",
    "    \n",
    "    # Create synthetic satellite-like data\n",
    "    x = np.linspace(20, 28, 800)\n",
    "    y = np.linspace(54, 60, 600)\n",
    "    \n",
    "    # Simulate NDVI-like patterns\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    ndvi_pattern = 0.3 + 0.4 * np.sin(xx * 3) * np.cos(yy * 2) + 0.1 * np.random.random((600, 800))\n",
    "    ndvi_pattern = np.clip(ndvi_pattern, -1, 1)\n",
    "    \n",
    "    # Create xarray dataset\n",
    "    synthetic_data = xr.Dataset({\n",
    "        'ndvi': (['y', 'x'], ndvi_pattern)\n",
    "    }, coords={\n",
    "        'x': x,\n",
    "        'y': y\n",
    "    })\n",
    "    \n",
    "    print(f\"Synthetic dataset created: {synthetic_data.ndvi.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate cloud-native processing concepts\n",
    "print(\"Cloud-native processing benefits:\")\n",
    "print(\"\\n1. Lazy Loading:\")\n",
    "print(\"   - Data is only loaded when needed (when .compute() is called)\")\n",
    "print(\"   - Allows working with datasets larger than memory\")\n",
    "\n",
    "print(\"\\n2. Chunked Processing:\")\n",
    "print(\"   - Large arrays are split into smaller chunks\")\n",
    "print(\"   - Each chunk can be processed in parallel\")\n",
    "print(\"   - Scales from local machine to cloud clusters\")\n",
    "\n",
    "print(\"\\n3. Efficient I/O:\")\n",
    "print(\"   - Only read the data you need (spatial/temporal subsetting)\")\n",
    "print(\"   - Compressed formats reduce transfer time\")\n",
    "print(\"   - Co-location of data and compute reduces latency\")\n",
    "\n",
    "# Demonstrate lazy operations\n",
    "if 'synthetic_data' in locals():\n",
    "    print(\"\\nDemonstrating lazy operations:\")\n",
    "    \n",
    "    # Create a lazy operation\n",
    "    subset = synthetic_data.ndvi.sel(x=slice(22, 26), y=slice(56, 58))\n",
    "    mean_value = subset.mean()\n",
    "    \n",
    "    print(f\"Created lazy operation - no computation yet\")\n",
    "    print(f\"Result type: {type(mean_value.data)}\")\n",
    "    \n",
    "    # Now compute the result\n",
    "    computed_mean = mean_value.compute()\n",
    "    print(f\"Computed result: {computed_mean.values:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 4: Parallel Processing with Dask\n",
    "\n",
    "Let's demonstrate how Dask enables parallel processing for large-scale EO applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger synthetic dataset to demonstrate parallel processing\n",
    "print(\"Creating large synthetic dataset...\")\n",
    "\n",
    "# Simulate a time series of satellite images\n",
    "n_times = 50\n",
    "height, width = 2000, 2000\n",
    "dates = pd.date_range('2023-01-01', periods=n_times, freq='W')\n",
    "\n",
    "# Create chunked dask array (this is key for parallel processing)\n",
    "chunks = (10, 500, 500)  # (time, y, x)\n",
    "data_array = da.random.random((n_times, height, width), chunks=chunks)\n",
    "\n",
    "# Add some realistic patterns\n",
    "# Seasonal trend\n",
    "seasonal_trend = np.sin(np.arange(n_times) * 2 * np.pi / 52) * 0.3 + 0.5\n",
    "data_array = data_array + seasonal_trend[:, None, None]\n",
    "\n",
    "# Create xarray dataset with proper coordinates\n",
    "x_coords = np.linspace(20, 30, width)\n",
    "y_coords = np.linspace(50, 60, height)\n",
    "\n",
    "large_dataset = xr.Dataset({\n",
    "    'vegetation_index': (['time', 'y', 'x'], data_array)\n",
    "}, coords={\n",
    "    'time': dates,\n",
    "    'y': y_coords,\n",
    "    'x': x_coords\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {large_dataset.vegetation_index.shape}\")\n",
    "print(f\"Chunk sizes: {large_dataset.vegetation_index.chunks}\")\n",
    "print(f\"Total size: {large_dataset.vegetation_index.nbytes / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate parallel operations\n",
    "print(\"Performing parallel computations...\")\n",
    "\n",
    "# 1. Temporal statistics (computed in parallel across time)\n",
    "annual_mean = large_dataset.vegetation_index.mean(dim='time')\n",
    "annual_std = large_dataset.vegetation_index.std(dim='time')\n",
    "\n",
    "# 2. Spatial statistics (computed in parallel across space)\n",
    "spatial_mean = large_dataset.vegetation_index.mean(dim=['x', 'y'])\n",
    "\n",
    "# 3. Moving window operations\n",
    "monthly_mean = large_dataset.vegetation_index.rolling(time=4, center=True).mean()\n",
    "\n",
    "print(\"All operations defined (lazy evaluation)\")\n",
    "print(\"\\nComputing results...\")\n",
    "\n",
    "# Show task graph information\n",
    "print(f\"Annual mean task graph: {annual_mean.data.npartitions} partitions\")\n",
    "\n",
    "# Compute one result to demonstrate\n",
    "result = spatial_mean.compute()\n",
    "print(f\"Spatial mean computed: {len(result)} time points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the parallel processing results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot time series\n",
    "spatial_mean.plot(ax=ax1, x='time')\n",
    "ax1.set_title('Regional Mean Vegetation Index Over Time')\n",
    "ax1.set_ylabel('Vegetation Index')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot a spatial slice (compute a small subset)\n",
    "subset = large_dataset.vegetation_index.isel(time=25, x=slice(0, 500), y=slice(0, 500))\n",
    "computed_subset = subset.compute()\n",
    "\n",
    "im = computed_subset.plot(ax=ax2, cmap='viridis', add_colorbar=False)\n",
    "ax2.set_title('Spatial Pattern (Week 25)')\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.colorbar(im, ax=ax2, label='Vegetation Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey benefits of parallel processing:\")\n",
    "print(\"✅ Process datasets larger than memory\")\n",
    "print(\"✅ Automatic parallelization across CPU cores\")\n",
    "print(\"✅ Scales to distributed computing clusters\")\n",
    "print(\"✅ Familiar NumPy/Pandas-like interface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 5: Integration with ESA Services\n",
    "\n",
    "Let's demonstrate how these concepts integrate with ESA's operational services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate connection to ESA services\n",
    "print(\"ESA Service Integration Examples:\")\n",
    "print(\"\\n1. Copernicus Data Space Ecosystem:\")\n",
    "print(\"   - Direct access to Sentinel missions data\")\n",
    "print(\"   - Integrated processing environment\")\n",
    "print(\"   - No data download required\")\n",
    "\n",
    "print(\"\\n2. EOPF (Earth Observation Processing Framework):\")\n",
    "print(\"   - Harmonized access across missions\")\n",
    "print(\"   - Zarr format for cloud optimization\")\n",
    "print(\"   - Consistent APIs and data models\")\n",
    "\n",
    "print(\"\\n3. Cubes & Clouds Educational Platform:\")\n",
    "print(\"   - Hands-on learning environment\")\n",
    "print(\"   - Community collaboration projects\")\n",
    "print(\"   - Best practices and workflows\")\n",
    "\n",
    "# Example of how you would access these services\n",
    "service_examples = {\n",
    "    \"Copernicus Data Space\": \"https://dataspace.copernicus.eu/\",\n",
    "    \"EOPF Sample Service\": \"https://zarr.eopf.copernicus.eu/\",\n",
    "    \"Cubes & Clouds\": \"https://eo-college.org/courses/cubes-and-clouds\",\n",
    "    \"STAC Browser\": \"https://stac.browser.user.eopf.eodc.eu\"\n",
    "}\n",
    "\n",
    "print(\"\\nService Access Points:\")\n",
    "for service, url in service_examples.items():\n",
    "    print(f\"  {service}: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate workflow integration concepts\n",
    "print(\"Complete Workflow Integration:\")\n",
    "print(\"\\n📡 Data Discovery (STAC)\")\n",
    "print(\"   ↓\")\n",
    "print(\"☁️  Cloud Access (COG/Zarr)\")\n",
    "print(\"   ↓\")\n",
    "print(\"🐍 Python Processing (Xarray/Dask)\")\n",
    "print(\"   ↓\")\n",
    "print(\"📊 Analysis & Visualization\")\n",
    "print(\"   ↓\")\n",
    "print(\"🔄 Sharing & Collaboration (FAIR)\")\n",
    "\n",
    "# Example workflow code structure\n",
    "workflow_example = '''\n",
    "# 1. Discovery\n",
    "items = catalog.search(collections=[\"sentinel-2\"], bbox=bbox, datetime=date_range)\n",
    "\n",
    "# 2. Access\n",
    "dataset = xr.open_dataset(items[0].assets['data'].href, engine='zarr')\n",
    "\n",
    "# 3. Process\n",
    "ndvi = (dataset.nir - dataset.red) / (dataset.nir + dataset.red)\n",
    "result = ndvi.compute()\n",
    "\n",
    "# 4. Share\n",
    "result.to_netcdf('ndvi_analysis.nc')\n",
    "'''\n",
    "\n",
    "print(\"\\nExample workflow structure:\")\n",
    "print(workflow_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "This demonstration notebook has shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of what we've demonstrated\n",
    "takeaways = {\n",
    "    \"Data Cube Concepts\": [\n",
    "        \"N-dimensional labeled arrays with Xarray\",\n",
    "        \"Efficient slicing and selection operations\",\n",
    "        \"Built-in operations (groupby, resample, etc.)\"\n",
    "    ],\n",
    "    \"Cloud-Native Access\": [\n",
    "        \"STAC for standardized data discovery\",\n",
    "        \"Lazy loading and chunked processing\",\n",
    "        \"Efficient I/O with cloud-optimized formats\"\n",
    "    ],\n",
    "    \"Parallel Processing\": [\n",
    "        \"Dask for automatic parallelization\",\n",
    "        \"Scaling from local to distributed computing\",\n",
    "        \"Memory-efficient operations on large datasets\"\n",
    "    ],\n",
    "    \"ESA Integration\": [\n",
    "        \"Real-world operational services\",\n",
    "        \"Community-driven development\",\n",
    "        \"End-to-end workflow examples\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"🎯 Key Takeaways from Session 1:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for category, points in takeaways.items():\n",
    "    print(f\"\\n📌 {category}:\")\n",
    "    for point in points:\n",
    "        print(f\"   • {point}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"🔜 Next: Session 2 - FAIR Data & Open Science\")\n",
    "print(\"   We'll build on these technical foundations to create\")\n",
    "print(\"   sustainable, collaborative EO workflows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "For continued learning and exploration:\n",
    "\n",
    "### Documentation\n",
    "- **[Xarray User Guide](https://docs.xarray.dev/en/stable/user-guide/index.html)**\n",
    "- **[Dask Documentation](https://docs.dask.org/en/stable/)**\n",
    "- **[STAC Specification](https://stacspec.org/)**\n",
    "- **[Pangeo Tutorials](https://pangeo.io/tutorials.html)**\n",
    "\n",
    "### Hands-on Practice\n",
    "- **[EOPF Sample Notebooks](https://eopf-sample-service.github.io/eopf-sample-notebooks/)**\n",
    "- **[Cubes & Clouds Course](https://eo-college.org/courses/cubes-and-clouds)**\n",
    "- **[Pangeo Gallery](https://gallery.pangeo.io/)**\n",
    "\n",
    "### Community\n",
    "- **[Pangeo Discourse Forum](https://discourse.pangeo.io/)**\n",
    "- **[EO College Community](https://eo-college.org/)**\n",
    "- **[STAC Community](https://github.com/radiantearth/stac-spec)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
